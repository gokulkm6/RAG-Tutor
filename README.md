# RAG Tutor

RAG Tutor is a locally deployable Retrieval-Augmented Generation (RAG) based voice-enabled tutor that uses LangChain, Hugging Face, and FAISS to answer questions from your own documents â€” completely offline.  
It combines speech recognition, retrieval-based reasoning, and local language model inference to create an interactive tutoring experience.

---

## Features

- Voice Interaction â€” Speak your query; the AI listens and replies vocally.  
- RAG Pipeline â€” Retrieves relevant context from your local documents.  
- LLM-Powered Tutoring â€” Uses a lightweight summarization model (BART) for response generation.  
- Local FAISS Vector Store â€” No cloud dependency.  
- Emotion-Driven Mascot â€” Responds visually with expressive emotions.  
- FastAPI Backend + React Frontend â€” Modern, modular, and easily deployable.  
- Embeddings via HuggingFace â€” SentenceTransformer model for semantic similarity.  
- Fully Offline â€” Ideal for restricted environments or educational setups.

---

## Tech Stack

| Layer | Technology |
|-------|-------------|
| Frontend | React + TailwindCSS + SpeechSynthesis + Web Speech API |
| Backend | FastAPI |
| RAG Engine | LangChain + FAISS + Hugging Face Pipelines |
| Embeddings | sentence-transformers/all-MiniLM-L6-v2 |
| Model | facebook/bart-large-cnn |
| Environment | Python 3.10+, Node 18+ |

---

## Folder Structure

```
RAG Tutor/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ main.py # FastAPI app entry
â”‚ â”‚ â”œâ”€â”€ rag_engine.py # Core RAG logic
â”‚ â”‚ â””â”€â”€ vectorstore/
â”‚ â”‚ â”œâ”€â”€ build_vectorstore.py # Script to build FAISS index
â”‚ â”‚ â””â”€â”€ faiss_index/ # Stored vector database
â”‚ â”œâ”€â”€ docs/ # Local text documents for RAG
â”‚ â””â”€â”€ venv/ # Virtual environment (Python)
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ public/ # Static assets
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ components/
â”‚ â”‚ â”‚ â”œâ”€â”€ Mascot.jsx # Voice mascot with speech + emotion
â”‚ â”‚ â”‚ â””â”€â”€ ChatWindow.jsx # Chat UI component
â”‚ â”‚ â”œâ”€â”€ App.js # Root app component
â”‚ â”‚ â”œâ”€â”€ index.js # Entry point
â”‚ â”‚ â””â”€â”€ index.css # Styling
â”‚ â””â”€â”€ package.json # React dependencies
â”‚
â””â”€â”€ README.md

```

---

## Installation & Setup

### 1ï¸. Backend Setup
cd backend
python -m venv venv
venv\Scriptsctivate # On Linux: source venv/bin/activate
pip install -r requirements.txt

text

Required dependencies:
fastapi
uvicorn
langchain
langchain-community
langchain-core
langchain-huggingface
faiss-cpu
transformers
sentence-transformers

text

### 2. Add Documents
Place your `.txt` files inside:
backend/docs/

text

Example:
backend/docs/intro.txt
backend/docs/langchain_overview.txt

text

### 3. Build the Vector Store
python backend/app/vectorstore/build_vectorstore.py

text

You should see:
FAISS index saved at: backend/app/vectorstore/faiss_index

text

### 4. Start Backend Server
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000

text

Access API docs at:  
http://127.0.0.1:8000/docs

Test API:
curl -X POST "http://127.0.0.1:8000/chat"
-H "Content-Type: application/json"
-d "{"query": "What is LangChain?"}"

text

### 5. Frontend Setup
cd frontend
npm install
npm start

text

Frontend runs at:  
http://localhost:3000

---

## API Schema

POST /chat

Request:
{
"query": "What is LangChain?"
}

text

Response:
{
"text": "LangChain is a framework for building applications powered by language models.",
"emotion": "explaining",
"sources": ["intro.txt"]
}

text

---

## Key Learnings & Challenges

### Problem Understanding
Building a self-contained AI tutor that interacts using voice and contextually reasons from local documents.

### Step-by-Step Breakdown
- Document ingestion â†’ chunking â†’ embedding â†’ FAISS indexing  
- Query retrieval using semantic similarity  
- Local LLM (BART) for summarization-based generation  
- Voice input & synthesis integration in React  
- Emotion-driven mascot rendering  

### Key Decisions
- Used FAISS for fast offline similarity search.  
- Adopted HuggingFacePipeline for local LLM calls.  
- Simplified RAG logic for low-latency local inference.

### Challenges & Learnings
- Model compatibility fixed via HuggingFacePipeline.invoke  
- CORS and 422 schema mismatches resolved in FastAPI  

---

## ğŸ“˜ License

MIT License Â© 2025 Gokul
