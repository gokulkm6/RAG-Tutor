# ğŸ§  RAG Tutor â€” Offline Voice-Interactive AI Learning Assistant

RAG Tutor is a locally deployable Retrieval-Augmented Generation (RAG) based voice-enabled tutor that uses LangChain, Hugging Face, and FAISS to answer questions from your own documents â€” completely offline.  
It combines speech recognition, retrieval-based reasoning, and local language model inference to create an interactive tutoring experience.

---

## ğŸš€ Features

- ğŸ™ï¸ Voice Interaction â€” Speak your query; the AI listens and replies vocally.  
- ğŸ§© RAG Pipeline (Offline) â€” Retrieves relevant context from your local documents.  
- ğŸ’¬ LLM-Powered Tutoring â€” Uses a lightweight summarization model (BART) for response generation.  
- ğŸ’¾ Local FAISS Vector Store â€” No cloud dependency.  
- ğŸ˜„ Emotion-Driven Mascot â€” Responds visually with expressive emotions.  
- âš¡ FastAPI Backend + React Frontend â€” Modern, modular, and easily deployable.  
- ğŸ§  Embeddings via HuggingFace â€” SentenceTransformer model for semantic similarity.  
- ğŸ”’ Fully Offline â€” Ideal for restricted environments or educational setups.

---

## ğŸ§± Tech Stack

| Layer | Technology |
|-------|-------------|
| Frontend | React + TailwindCSS + SpeechSynthesis + Web Speech API |
| Backend | FastAPI |
| RAG Engine | LangChain + FAISS + Hugging Face Pipelines |
| Embeddings | sentence-transformers/all-MiniLM-L6-v2 |
| Model | facebook/bart-large-cnn |
| Environment | Python 3.10+, Node 18+ |

---

## ğŸ“ Folder Structure

RAG Tutor/
â”‚
â”œâ”€â”€ backend/
â”‚ â”œâ”€â”€ app/
â”‚ â”‚ â”œâ”€â”€ main.py # FastAPI app entry
â”‚ â”‚ â”œâ”€â”€ rag_engine.py # Core RAG logic
â”‚ â”‚ â””â”€â”€ vectorstore/
â”‚ â”‚ â”œâ”€â”€ build_vectorstore.py # Script to build FAISS index
â”‚ â”‚ â””â”€â”€ faiss_index/ # Stored vector database
â”‚ â”œâ”€â”€ docs/ # Local text documents for RAG
â”‚ â””â”€â”€ venv/ # Virtual environment (Python)
â”‚
â”œâ”€â”€ frontend/
â”‚ â”œâ”€â”€ public/ # Static assets
â”‚ â”œâ”€â”€ src/
â”‚ â”‚ â”œâ”€â”€ components/
â”‚ â”‚ â”‚ â”œâ”€â”€ Mascot.jsx # Voice mascot with speech + emotion
â”‚ â”‚ â”‚ â””â”€â”€ ChatWindow.jsx # Chat UI component
â”‚ â”‚ â”œâ”€â”€ App.js # Root app component
â”‚ â”‚ â”œâ”€â”€ index.js # Entry point
â”‚ â”‚ â””â”€â”€ index.css # Styling
â”‚ â””â”€â”€ package.json # React dependencies
â”‚
â””â”€â”€ README.md

text

---

## âš™ï¸ Installation & Setup

### 1ï¸âƒ£ Backend Setup
cd backend
python -m venv venv
venv\Scriptsctivate # On Linux: source venv/bin/activate
pip install -r requirements.txt

text

Required dependencies:
fastapi
uvicorn
langchain
langchain-community
langchain-core
langchain-huggingface
faiss-cpu
transformers
sentence-transformers

text

### 2ï¸âƒ£ Add Documents
Place your `.txt` files inside:
backend/docs/

text

Example:
backend/docs/intro.txt
backend/docs/langchain_overview.txt

text

### 3ï¸âƒ£ Build the Vector Store
python backend/app/vectorstore/build_vectorstore.py

text

You should see:
FAISS index saved at: backend/app/vectorstore/faiss_index

text

### 4ï¸âƒ£ Start Backend Server
uvicorn app.main:app --reload --host 127.0.0.1 --port 8000

text

Access API docs at:  
http://127.0.0.1:8000/docs

Test API:
curl -X POST "http://127.0.0.1:8000/chat"
-H "Content-Type: application/json"
-d "{"query": "What is LangChain?"}"

text

### 5ï¸âƒ£ Frontend Setup
cd frontend
npm install
npm start

text

Frontend runs at:  
http://localhost:3000

---

## ğŸ§© Example Query

You: â€œWhat is machine learning?â€  
Tutor: â€œMachine learning is a subfield of AI that enables systems to learn patterns from data and make predictions without explicit programming.â€

---

## ğŸ› ï¸ Troubleshooting

| Error | Cause | Fix |
|-------|--------|-----|
| Docs folder not found | Missing or wrong path | Ensure backend/docs/ exists and contains .txt files |
| could not open index.faiss for reading | FAISS index missing | Re-run build_vectorstore.py |
| 'VectorStoreRetriever' object has no attribute get_relevant_documents | Wrong retriever usage | Use retriever.get_relevant_documents(query) |
| 'HuggingFacePipeline' object is not callable | Direct model call issue | Use llm.invoke(prompt) instead |
| Output contains leftover text | Old prompt template | Update answer_query_offline() with cleaned prompt |
| Frontend empty output | Backend returned empty JSON | Check if /chat route returns text properly |
| TypeError: Failed to fetch | CORS issue | Add FastAPI CORS middleware for localhost:3000 |

---

## ğŸ§  Optimization & Edge Deployment

- Model Quantization: Use ONNX or int8 quantization for edge devices.  
- Linux Cloud Deployment:  
  - Install OpenCV manually if dependency conflicts occur:  
    ```
    sudo apt install libgl1
    ```
  - Use gunicorn + uvicorn.workers.UvicornWorker for production.  
- Offline Mode:  
  Pre-download all models and embeddings using:
transformers-cli download

text

---

## ğŸ§© API Schema

POST /chat

Request:
{
"query": "What is LangChain?"
}

text

Response:
{
"text": "LangChain is a framework for building applications powered by language models.",
"emotion": "explaining",
"sources": ["intro.txt"]
}

text

---

## ğŸ§  Key Learnings & Challenges

### Problem Understanding
Building a self-contained AI tutor that interacts using voice and contextually reasons from local documents.

### Step-by-Step Breakdown
- Document ingestion â†’ chunking â†’ embedding â†’ FAISS indexing  
- Query retrieval using semantic similarity  
- Local LLM (BART) for summarization-based generation  
- Voice input & synthesis integration in React  
- Emotion-driven mascot rendering  

### Key Decisions
- Used FAISS for fast offline similarity search.  
- Adopted HuggingFacePipeline for local LLM calls.  
- Simplified RAG logic for low-latency local inference.

### Challenges & Learnings
- Model compatibility fixed via HuggingFacePipeline.invoke  
- CORS and 422 schema mismatches resolved in FastAPI  
- Prompt leakage removed via cleaner PromptTemplate  
- Manual OpenCV fix on Linux (libGL.so missing)  

### Optimization for Edge
- Convert embeddings to HNSWlib or SQLite vector store.  
- Replace bart-large-cnn with t5-small or flan-t5-base for low-memory systems.  
- Bundle model weights locally using transformers-cli download.

---

## ğŸ“˜ License

MIT License Â© 2025 Gokul
